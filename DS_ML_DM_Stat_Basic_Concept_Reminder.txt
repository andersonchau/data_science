Ref : 
Statistical Method for Machine Learning -- Jason Brownlee


Stat/DS/ML/DM Terms/Concept glossory for reminder.
 
( For my own reminder purpose after studying Stat/DS/ML/DM topics as a support for further studies
, readers are encourage to first study stat/DS/ML/DM text/papers/stackoverflow/stackexchange on their own because the
following information may not be accurate and not self-contained. )
** I also do not guarantee the below is correct **  
==========================================================================================================================
ML/DM difference :
-- use same technique ( SVM, LR, Logistic R ... )
ML - we program something (predict a model for future preduction) using data
DM - we use data to build model to create knowledge ( knowledge discovery)
==========================================================================================================================
==========================================================================================================================
Statistics -- how to collect data, interpret data, reach conclusion about data. 
Descriptive Stat -- mean/var/skewness to discribe data, visualization 
Inferential Stat -- estimation , hypothesis testing
ML(Predictive modeling) vs Stat(Stat learning) -- use algo to model data vs studies of maths of model and goodness of fit.
Exploratory data analysis -- summarization and visualization of data. 
Data Mining -- finding patterns/relationship in data.


RV :
A numerical variable, having it values comes in an associated probability.
==========================================================================================================================
Independent, Identically distributed(iid):
Independent --  different samples have no correlations , formal P(x and y) = P(x)*P(y)
Identically distributed -- have the same distribution 
==========================================================================================================================
Central Limit Theorem (proof omitted): 
When sample size is large enough, the sample means will comes in Normal distribution with (Population of any distribution)
sample mean = population mean 
and sample SD = PSD/sqrt(n)
==========================================================================================================================
Pearson coefficient :
-- -1 to 1, 1 large LINEAR relationship between 2 RV, 0 independent.
-- but easily affected by outliers. 
==========================================================================================================================














============================================================================================================================
PCA : 
-- problem : too many features , we want to combine some features into one by linear combination
-- But we still want to maximize the variance of new feature,so that the one component still collects the most "uniqueness" from the data set.
( to minimize the reconstruction error ) 
-- in his case transformed vectors will lose the min. consistuent elements characteristics (compressed lesser other-dimension ) 
-- it is found by finding the project of "principle axis" of the data in features 
and features are projected into this axis to form the new feature. 
-- it can be proved that max var. is equivalent to finding max. eigenvalue and it associated eigenvector ( principal axis ). 

==========================================================================================================================
Greedy Algorithm : 
-- We have no algo to find an optimal solution, brute force is too costly. 
-- We define an objective(largest reduction in entropy, which we conform to and and make the best of it. 
-- Greedy Algorithm generally not finding optimal solution, but based on the objective
Dynamic programming is used with Greedy Algorithm to break down bigger problem into smaller problem and fix it. 

==========================================================================================================================

Shannon Entropy Concept ( for decision tree ID3、C4.5、C5.0) :

How Entropy formula relates to certainty. 
-- Surpurise ( events of low probability ), 
(1) Measure the EXPECTED average min. amount of information (number of bits) used to store a distribution of event, 
to save bits, we generally use smaller number of bits to indicate higher change event (e.g. 01 ) and larger number 
of bits to denote smaller chance event (e.g. 00100101 ). This can be calculated by Shannon Entropy formula. 
(2) evenly distributed events(10*10%) (no surprise/uncertain) generally has highest entropy 
( we cannot do optimization to assign higher number of bits to lesser chance events ), more certain cases(80%,10%,10%..) use lesser 
bytes. 
=> Entropy formula measure the uncertainty of event distribution. => higher entropy, higher uncertainty 

How certainty relates to Decision Tree.
IG (Information Gain) => amount reduction in entropy => amount of uncertainty removed.
With this splitting, we are in a more certain position make decision.


Some Decision tree are the greedy algorithm aiming to minimize the largest entropy (greatest information gain) first. 
=> DT is to do splitting on the feature which provides highest promotion of certainty. 
e.g. P(C_1|A_1) = 0.4/P(C_2|A_1) = 0.6/P(C_1|A_2) = 0.4/P(C_2|A_2) = 0.6 
<-- less certain , even after answering this question, we do not gain much information. 

P(C_1|B_1) = 0.01,P(C_2|B_1) = 0.99,P(C_1|B_2) = 0.99,P(C_2|B_2) = 0.01 
=> We will choose Splitting on B instead of A first, and we probably will discard the splitting on A. 

e.g. features, M/F , Tall/Short, muscular/thin => play basketball 
M/F less IG , T/S highest IG, M/T middle IG
(same percentage of M/F player basketball,  short people generally wont play  )
Tree may be 
Short -> not player , Tall -> next 
Tall+Thin -> not player , Tall+muscular -> player 
M/F too low IG , not considered

============================================================================================================================

Gini impurity (CART) , usage : similar to Entropy 
This is the probably of a class of object randomly put into the distribution, what is the probability of misclassification 
higher => more uncertain
in this case : 1-p1^2 - p2^2 ......  , 
==========================================================================================================================
https://www.jmp.com/en_hk/statistics-knowledge-portal/t-test/two-sample-t-test.html

Hypothesis Testing (with t-test,z-test,sign-test) :

H0(Null Hypothesis) - Assumption failed to be rejected
H1(Alternative Hypothesis) - Assumption rejected at some level significance 

p-value :
The probability that the sample gives such a (or more extreme) result, given the condition that H0 is true. 
It is tested against significant level (e.g. 95%/99% ) for Hypothesis testing 
p-value obtained by sample is used to compare against a critical value of a sig. level for H-Test
p-value<critical value => not sig. result , failed to reject H0
p-value>critical value => sig. result , reject H0
e.g. sample = 100 , mean = 25 min , p-value = P(mean > 25 min | H0 true )

Assume Sig. level = 95%
Type I Error : Incorrect rejection of a true H0, false positive 
Type II Error : H1 is in fact true but we failed to reject H0, false negative


T-test : ( sample size < 30 , or (unknown population var AND sample size large) or known normal distribution )
1-sample-test : test whether mean of single population = target value ( is mean this sample of this population = 30) 

2-sample-test : test whether means of 2 population are different ( does mean of height of female diff from mean of height of male )
Paired T : Test whether difference between 2 dependent = target value ( does mean of weight diff after taking weight loss pill )

python : ttest_rel(data1, data2) // from scipy.stats import ttest_rel

Z-test( assumed normal distribution , used when known population variance or sample size >= 30 )
1-sample-test: test whether sample mean =/= population mean 
2-sample-test: test wether means of 2 samples are different.

t-test use sample SD , z-test used population SD

ANOVA
one-way : H0: all sample independent distribution are equals , H1: one or more sample mean(s) is/are not equal
python : stat, p = f_oneway(data1, data2, data3) // from scipy.stats import f_oneway
Repeated Measures ANOVA Test : H0: aAll paired sample distributions are equal , H1: One or more paired sample distributions are not equal.
==========================================================================================================================

Chi square test (Goodness of fit) 
full example : https://stattrek.com/chi-square-test/goodness-of-fit.aspx
Given categorical variable , it is used in Hypothesis test whether the sample data consistent(H0) with a specified distribution or not (Ha)
e.g. Merchant claimed that the draw have 10% gold, 20% silver, 70% bronze coin , we have 2% gold, 8 % silver, 92% bronze coin in sample 
 
 ==========================================================================================================================

Chi square test (Independence ) :
full example : https://stattrek.com/chi-square-test/independence.aspx
-- applicable to X2 test apply to categorical variable, each sample size happens at least n>=5, sample is simple random sampleing 
-- case similar to GoF test, just to check whether 2 distribution are depenent or not. 

One-way Test : whether in a group of distribution , whether means are different by examining the variance.
ref : https://online.stat.psu.edu/stat500/lesson/10/10.1

==========================================================================================================================
Association Rule Mining , basket analysis : 
confidence  :
lift :
support :  
==========================================================================================================================
Maximum Likelihood Estimation (MLE):
When we have a set of sample data, we want to find the best parameter of a model (e.g. mean, standard deviation ) which is best fit to this 
observation ( sampled data). best chance (max. likehood) that fit this model.
==========================================================================================================================
Bias/Variance Analysis :

==========================================================================================================================
Unbias Estimator :
==========================================================================================================================
Linear Discriminant Analysis :
==========================================================================================================================
Generative vs Discrminative Learning Model for classification  (ref. CS229 notes )
D - example : Logistic Regression.
G - example : Naive Bayes Classifier 
suppose 2 classes y1, y2 cases, x be features
Discriminative : to classify y based on x <=> to see p(y1|x) or p(y2|x) is greater for hypothesis. 
Generative : Approach to find p(y|x), instead of finding p(y|x) directly , 
model p(x|y) (given known classes, observe the distribution of its features) ,then use  p(x|y)p(y)/p(x) to find p(y|x)
p(x) can be omitted because argmax y p(y|x) = argmax y p(x|y)p(y) = argmax y (p(x AND y))
which means : In the modeling process , if want to find max. p(y|x) (i.e. remember we estimate to get theta by maximizing P(theta|x) in MLE )
it is the same as finding max value of (p(x AND y)) during our observation.


p(y|x) means given features, the distribution of p(y) which is h(x)
p(x|y) found by we have samples which is classified , we observe their features. 
y = elephant ,man, x = height 

 