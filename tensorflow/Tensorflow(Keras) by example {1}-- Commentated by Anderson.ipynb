{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ref : https://www.tensorflow.org/tutorials/quickstart/beginner \n",
    "For learning TF with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "# print(type(x_train)) # numpy.ndarray\n",
    "# print(x_train.shape) #(60000, 28, 28), meaning, 60000 samples, 28x28 \n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "#Normalize the data to from 0 to 1 using broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the NN structure :<br><br>\n",
    "Sequential -> steps are run sequentially, one by one<br><br>\n",
    "Flatten -> convert (28,28) data into (28*28,1) data<br><br>\n",
    "Using relu as activation function, other options(tanh, sigmoid{not-so-good for stabilizing gradient})<br><br>\n",
    "Dropout -> using Dropout at prob. 0.2 , to reduce overfitting.<br><br>\n",
    "Dense -> fully connected layer with 128 neurons<br><br>\n",
    "ref : https://sparrow.dev/keras-dense-layer/ <br><br>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- x_train is (60000,28,28), x_train[:1] is (1,28,28)\n",
    "- predictions is returned in log odds (i.e. log(p/(1-p)) )\n",
    "- This predictions is fed to the last softmax layer which take log-odds as input. \n",
    "- softmax() converts log odds(logits) to normal probability\n",
    "- numpy() convert data from tensor data type to numpy data type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28)\n",
      "[[ 0.14525738  0.44951153 -0.35766387 -0.17903815 -0.33537504  0.29884958\n",
      "  -0.11013418  0.4570856  -0.30038756  0.43196672]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.10437541, 0.14149272, 0.06312223, 0.07546723, 0.06454494,\n",
       "        0.12170333, 0.08085057, 0.14256847, 0.06684317, 0.13903192]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(x_train[:1].shape)\n",
    "\n",
    "predictions = model(x_train[:1]).numpy() # note this model is untrained yet!!!!\n",
    "print(predictions)\n",
    "\n",
    "tf.nn.softmax(predictions).numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This define a loss function in Cross-entropy:\n",
    "- Least Square is an example of loss function, we want to find parameters to minimize the loss\n",
    "- Cross Entropy is the loss function for NN which compare the difference between 2 distributions, probably from hypothesis and sample distrbutions. \n",
    "- from_logits=True, input as logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# loss_fn(y_train[:1], predictions).numpy() # example of running "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define the model :\n",
    "\n",
    "- adam is the momentum based technique used in gradient descent to improve the speed of finding minimum, ref : https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/ \n",
    "- epoch is the number of times of feeding the full training set to the model \n",
    "https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9  , too low -> underfit, too high -> overfit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.2902 - accuracy: 0.9148\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1408 - accuracy: 0.9579\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 1ms/step - loss: 0.1063 - accuracy: 0.9678\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0879 - accuracy: 0.9728\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 2s 1ms/step - loss: 0.0751 - accuracy: 0.9764\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x275b93c8f70>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=loss_fn,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5) # This step perform real training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 0.0707 - accuracy: 0.9774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07074465602636337, 0.977400004863739]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n",
       "array([[1.78269346e-07, 2.19373564e-09, 1.62400568e-06, 2.79981003e-04,\n",
       "        1.78932823e-12, 1.45183733e-07, 1.60922470e-14, 9.99708354e-01,\n",
       "        2.55418485e-07, 9.50978483e-06],\n",
       "       [7.81954952e-07, 6.09106850e-04, 9.99252975e-01, 1.34972550e-04,\n",
       "        4.04219193e-13, 1.35531320e-06, 1.30613870e-07, 7.09345693e-09,\n",
       "        5.84360635e-07, 5.83340139e-14],\n",
       "       [1.16320365e-07, 9.99193847e-01, 9.31574832e-05, 1.79182712e-06,\n",
       "        1.43747766e-05, 1.63772529e-06, 2.33122591e-05, 6.15166384e-04,\n",
       "        5.61877132e-05, 4.38096407e-07],\n",
       "       [9.99908447e-01, 3.33513372e-09, 2.67895630e-05, 1.68876042e-07,\n",
       "        1.79418453e-06, 1.32241976e-05, 3.64188622e-07, 2.36321807e-06,\n",
       "        1.68354219e-07, 4.66915371e-05],\n",
       "       [1.27034179e-08, 1.10635989e-09, 3.56521241e-06, 1.06545792e-06,\n",
       "        9.99246716e-01, 2.74419875e-07, 4.82346962e-08, 3.75298077e-06,\n",
       "        3.51200833e-06, 7.41071184e-04]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_model = tf.keras.Sequential([\n",
    "  model,\n",
    "  tf.keras.layers.Softmax()\n",
    "])\n",
    "probability_model(x_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
